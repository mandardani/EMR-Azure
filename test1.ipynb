{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38197a18-7e91-44c1-99cd-644747d6d972",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data=[['Brampton','Miss','Tor'],['', 'Toronto',''],['','','mississauga']]\n",
    "columns=['city1','city2','city3']\n",
    "df = spark.createDataFrame(data,columns)\n",
    "from pyspark.sql.functions import *\n",
    "df1= df.withColumn('city', coalesce(\n",
    "                    when(df.city1 ==\"\",None).otherwise(df.city1),\n",
    "                    when(df.city2 ==\"\",None).otherwise(df.city2),\n",
    "                    when(df.city3 ==\"\",None ).otherwise(df.city3)\n",
    "                                    ))\n",
    "display(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7fc6930-83f0-42a1-a9d2-0832b88446f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data1=[(1,\"Steve\"),(2,\"David\"),(3,\"John\"),(4,\"Shree\"),(5,\"Helen\")]\n",
    "data2=[(1,\"SQL\",90),(1,\"PySpark\",100),(2,\"SQL\",70),(2,\"PySpark\",60),(3,\"SQL\",30),(3,\"PySpark\",20),(4,\"SQL\",50),(4,\"PySpark\",50),(5,\"SQL\",45),(5,\"PySpark\",45)]\n",
    "\n",
    "schema1=[\"Id\",\"Name\"]\n",
    "schema2=[\"Id\",\"Subject\",\"Mark\"]\n",
    "\n",
    "df1=spark.createDataFrame(data1,schema1)\n",
    "df2=spark.createDataFrame(data2,schema2)\n",
    "df_ = df1.join(df2,df1.Id==df2.Id,how='inner').drop(df1.Id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd5b2a95-df51-463a-82ae-279901120475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_agg = df_.groupBy('id','Name').agg((sum(\"Mark\")/count(\"Mark\")).alias(\"percentage\"))\n",
    "#display(df_agg)\n",
    "df_fin = df_agg.select('id','Name','percentage',\n",
    "              when(df_agg.percentage>80,\"A\")\n",
    "             .when(df_agg.percentage>60,\"B\")\n",
    "             .when(df_agg.percentage>40,\"C\")\n",
    "             .when(df_agg.percentage>20,\"D\")\n",
    "             .otherwise(\"F\")\n",
    "             .alias(\"grade\"))\n",
    "display(df_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1d92f01-1051-455e-b2b6-bc3964c8d515",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *\n",
    "\n",
    "data1=[(1,\"A\",1000,\"IT\"),(2,\"B\",1500,\"IT\"),(3,\"C\",2500,\"IT\"),(4,\"D\",3000,\"HR\"),(5,\"E\",2000,\"HR\"),(6,\"F\",1000,\"HR\")\n",
    "       ,(7,\"G\",4000,\"Sales\"),(8,\"H\",4000,\"Sales\"),(9,\"I\",1000,\"Sales\"),(10,\"J\",2000,\"Sales\")]\n",
    "schema1=[\"EmpId\",\"EmpName\",\"Salary\",\"DeptName\"]\n",
    "df=spark.createDataFrame(data1,schema1)\n",
    "\n",
    "df_rank = df.withColumn(\"rank\",dense_rank().over(Window.partitionBy(\"DeptName\").orderBy(desc(\"Salary\"))))\n",
    "df_rank1 = df.select(\"*\", (dense_rank().over(Window.partitionBy(df.DeptName).orderBy(df.Salary.desc()))).alias(\"rank\"))\n",
    "display(df_rank1.filter(df_rank1.rank==2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045ad59f-4c95-4cb4-bbc1-98595c68d44f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#df.rdd.getNumPartition()\n",
    "df.repartition(10)\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "df.select(spark_partition_id().alias(\"partition\")).groupBy(\"partition\").count().show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e25a8be-0540-46df-a0d7-5e85602fd53e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# (modifiedBefore & modifiedAfter) file properties"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "test1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
